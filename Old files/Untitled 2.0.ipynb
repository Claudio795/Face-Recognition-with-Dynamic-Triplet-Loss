{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfQ9QvIwkH5U"
      },
      "source": [
        "# IA Project - Face Recognition with Dynamic Triplet Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNPVWoKykH5V"
      },
      "source": [
        "References: https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Learning_Local_Descriptors_With_a_CDF-Based_Dynamic_Soft_Margin_ICCV_2019_paper.pdf\n",
        "\n",
        "Dataset: http://vis-www.cs.umass.edu/lfw/#download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wht1Y6FEkH5W"
      },
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import collections\n",
        "import PIL.Image\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import os"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyN1fuC_kH5Z",
        "outputId": "22070dc1-8f6d-45bd-d42c-795c054f36be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(dev)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4qUcAnK--vF"
      },
      "source": [
        "### Pre-precessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEbUZchvkNi2"
      },
      "source": [
        "#path\n",
        "data_path = \"./LFW_DIR\"\n",
        "train_path = \"./data/train_pairs.txt\"\n",
        "test_path = \"./data/test_pairs.txt\"\n",
        "people_path = \"./data/people.txt\""
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndhx62qLkH5c"
      },
      "source": [
        "norm_mean = (0.485, 0.456, 0.406)\n",
        "norm_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(250),  # make 250x250\n",
        "    T.CenterCrop(150),   # then take 150x150 center crop\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(norm_mean, norm_std),\n",
        "])\n",
        "\n",
        "train_transform = T.Compose([\n",
        "    # per il training, la differenza risiede nel RandomCrop al posto del CenterCrop\n",
        "    # e nell'aggiunta dell'RandomHorizontalFlip\n",
        "    T.Resize(250),\n",
        "    T.RandomCrop(150),\n",
        "    T.RandomHorizontalFlip(p=0.5), # p è la probabilità che l'immagine venga ruotata\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(norm_mean, norm_std),\n",
        "])"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t5xsMwF--vU"
      },
      "source": [
        "def read_pairs(pairs_path):\n",
        "    pairs = []\n",
        "    with open(pairs_path, 'r') as file:\n",
        "        for line in file.readlines()[1:]:\n",
        "            pair = line.strip().split()\n",
        "            pairs.append(pair)\n",
        "    return pairs"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrA4LihA--vX",
        "outputId": "06c610ab-96a0-4c5a-cd10-285b1c4016c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Liste delle coppie di immagini per training e testing\n",
        "train_pairs = read_pairs(train_path)\n",
        "test_pairs = read_pairs(test_path)\n",
        "print(len(train_pairs), len(test_pairs))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2200 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0psCXAh3--va"
      },
      "source": [
        "#funzione per estrarre le immagini delle coppie\n",
        "def getImages(data_path, pairs):\n",
        "    skipped_pairs = 0\n",
        "    path_list = []\n",
        "    is_same_list = []\n",
        "    \n",
        "    for pair in pairs:\n",
        "        \n",
        "        if len(pair) == 3: # allora sono della stessa persona\n",
        "            path_first = os.path.join(data_path, pair[0], pair[0] + '_' + '%04d' % int(pair[1]) + '.jpg')\n",
        "            path_second = os.path.join(data_path, pair[0], pair[0] + '_' + '%04d' % int(pair[2]) + '.jpg')\n",
        "            is_same = True\n",
        "            \n",
        "        elif len(pair) == 4: # persone diverse\n",
        "            path_first = os.path.join(data_path, pair[0], pair[0] + '_' + '%04d' % int(pair[1]) + '.jpg')\n",
        "            path_second = os.path.join(data_path, pair[0], pair[0] + '_' + '%04d' % int(pair[3]) + '.jpg')\n",
        "            is_same = False\n",
        "            \n",
        "        # verifico che entrambe le immagini esistano ai dati path\n",
        "        if os.path.exists(path_first) and os.path.exists(path_second):\n",
        "            path_list += (path_first, path_second)\n",
        "            is_same_list.append(is_same)\n",
        "        else:\n",
        "            skipped_pairs += 1\n",
        "    \n",
        "    if skipped_pairs > 0:\n",
        "        print(f\"Pairs skipped: {skipped_pairs}\")\n",
        "    \n",
        "    return path_list, is_same_list"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yakBpd4r--vd",
        "outputId": "1d42fa8e-a05e-4b76-aa1b-cdddc8e05f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_path_list, is_same_train = getImages(data_path, train_pairs)\n",
        "test_path_list, is_same_test = getImages(data_path, test_pairs)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pairs skipped: 172\n",
            "Pairs skipped: 68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGkQorzu--vf"
      },
      "source": [
        "# definiamo una sottoclasse di dataset per LFW\n",
        "class LFWDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, path_list, is_same_list, transforms):\n",
        "        \n",
        "        self.path_list =  path_list\n",
        "        self.pair_label = is_same_list\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path_1 = self.path_list[index]\n",
        "        path_2 = self.path_list[index+1]\n",
        "        img1 = PIL.Image.open(path_1)\n",
        "        img2 = PIL.Image.open(path_2)\n",
        "        is_same = self.pair_label[index]\n",
        "        img1, img2 = self.transforms(img1), self.transforms(img2)\n",
        "        return img1, img2, is_same"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry5b_Irj--vi"
      },
      "source": [
        "# creiamo i dataset di training e testing\n",
        "train_dataset = LFWDataset(train_path_list, is_same_train, train_transform)\n",
        "test_dataset = LFWDataset(test_path_list, is_same_test, test_transform)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukm7Jetl--vl",
        "outputId": "7195ffb0-9011-47a6-ea19-c6415b838632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Compute dataset sizes\n",
        "num_train = len(train_dataset)\n",
        "num_test = len(test_dataset)\n",
        "\n",
        "print(f\"Num. training samples: {num_train}\")\n",
        "print(f\"Num. test samples:     {num_test}\")"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num. training samples: 4056\n",
            "Num. test samples:     1864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e7XhxBvCp9j"
      },
      "source": [
        "Dataset's samples structure:\n",
        "* 0: first image\n",
        "* 1: second image\n",
        "* 2: boolean that if it's a positive or a negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roAy13bMYyat",
        "outputId": "6b500e04-af7e-49bd-f8b1-0706e0c6e127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset[0][2]"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GF7wppJC1fC"
      },
      "source": [
        "Let's create the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwd8qW6CDPQk"
      },
      "source": [
        "# List of indexes on the training set\n",
        "train_idx = list(range(num_train))\n",
        "# List of indexes on the test set\n",
        "test_idx = list(range(num_test))"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT9Bt2XBC-MN"
      },
      "source": [
        "# Import\n",
        "import random\n",
        "# Shuffle training set\n",
        "random.shuffle(train_idx)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP2-zz7hDUiy"
      },
      "source": [
        "# Validation fraction\n",
        "val_frac = 0.1\n",
        "# Compute number of samples\n",
        "num_val = int(num_train*val_frac)\n",
        "num_train = num_train - num_val\n",
        "# Split training set\n",
        "val_idx = train_idx[num_train:]\n",
        "train_idx = train_idx[:num_train]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGWQYGsFDYYK"
      },
      "source": [
        "# Split train_dataset into training and validation\n",
        "val_dataset = Subset(train_dataset, val_idx)\n",
        "train_dataset = Subset(train_dataset, train_idx)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmtz3W2d--vo"
      },
      "source": [
        "# List of indexes on the training set\n",
        "train_idx = list(range(num_train))\n",
        "# List of indexes on the test set\n",
        "test_idx = list(range(num_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2iJLokB--v3"
      },
      "source": [
        "# Define loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=4, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=64, num_workers=4, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=64, num_workers=4, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TSXCW_J--v8"
      },
      "source": [
        "# Define dictionary of loaders\n",
        "loaders = {\"train\": train_loader,\n",
        "           \"val\": val_loader,\n",
        "           \"test\": test_loader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaH8gzRe--v_"
      },
      "source": [
        "### Triplet Loss implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FqUER20--wA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ljv1X0u--wD"
      },
      "source": [
        "### CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wcDytR--wE"
      },
      "source": [
        "# Model, CNN\n",
        "# import: neural networks, functional, optimization\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Classe del Model\n",
        "class CNN(nn.Module):\n",
        "    \n",
        "    # Constructor\n",
        "    def __init__(self):\n",
        "        # Call parent constructor\n",
        "        super().__init__();\n",
        "        # Create convolutional layers\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # Layer 1\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=0, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            \n",
        "            # Layer 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=0, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            \n",
        "            # Layer 3, con MaxPooling\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=0, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            \n",
        "            # Layer 4, Normalization e MaxPooling\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=0, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # Create fully-connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            # FC layer\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            # Classification layer\n",
        "            nn.Linear(1024, 10)\n",
        "        )\n",
        "\n",
        "    # Forward\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}