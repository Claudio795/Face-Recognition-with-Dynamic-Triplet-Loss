{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LFWDataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDqfRldZSDds"
      },
      "source": [
        "# IA Project - Face Recognition with Dynamic Triplet Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ceGeGASHIC"
      },
      "source": [
        "References: https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Learning_Local_Descriptors_With_a_CDF-Based_Dynamic_Soft_Margin_ICCV_2019_paper.pdf\n",
        "\n",
        "Dataset: http://vis-www.cs.umass.edu/lfw/#download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhRpOYJZR_ne"
      },
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import collections\n",
        "import PIL.Image\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKIejUZSSJq8",
        "outputId": "26463c46-8001-46db-942e-766acd5c64a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(dev)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkFSjhnQSLt7"
      },
      "source": [
        "### Pre-precessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdSQ4aNRSNyr"
      },
      "source": [
        "#path\n",
        "data_path = \"./LFW_DIR\"\n",
        "train_path = \"./data/train_pairs.txt\"\n",
        "test_path = \"./data/test_pairs.txt\"\n",
        "people_path = \"./data/people.txt\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywc2QkEkvM2c"
      },
      "source": [
        "norm_mean = (0.485, 0.456, 0.406)\n",
        "norm_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(250),  # make 250x250\n",
        "    T.CenterCrop(150),   # then take 150x150 center crop\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(norm_mean, norm_std),\n",
        "])\n",
        "\n",
        "train_transform = T.Compose([\n",
        "    T.Resize(250),\n",
        "    T.RandomCrop(150),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(norm_mean, norm_std),\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73VKzt_sSO87"
      },
      "source": [
        "def readPeople(people_path):\n",
        "  people_list = []\n",
        "  with open(people_path, 'r') as file:\n",
        "    for line in file.readlines():\n",
        "      person = line.strip().split()\n",
        "      people_list.append(person)\n",
        "  return people_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPQxtfRlTTgd"
      },
      "source": [
        "people_list= readPeople(people_path)\n",
        "# people_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgSGZx3JadkX"
      },
      "source": [
        "# cambiare i nomi con le labels\n",
        "def getLabeledImages(data_path, people_list):\n",
        "  labeledImages = {}\n",
        "  label = 0\n",
        "  \n",
        "\n",
        "  for person in people_list:\n",
        "    if int(person[1]) > 1:    # se ho pi√π di una immagine per persona\n",
        "      for img_id in range(1, int(person[1])+1):\n",
        "        path = os.path.join(data_path, person[0], person[0] + '_' + '%04d' % img_id + '.jpg')\n",
        "        labeledImages[path] = label\n",
        "      label += 1\n",
        "\n",
        "  return labeledImages"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxHovEu-U9rt",
        "outputId": "2f5e7af3-3956-44a4-b174-a93e90b7be90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "labeledImages = getLabeledImages(data_path, people_list)\n",
        "print(len(labeledImages.keys()))    # dizionario 'img_path: label'\n",
        "print(len(set(labeledImages.values())))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9164\n",
            "1680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JIPlIw7FfBg"
      },
      "source": [
        "class LFWDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, labeledImages, train=True, transform=None):\n",
        "    self.transform = transform\n",
        "    self.labeledImages = labeledImages\n",
        "    self.images = list(labeledImages.keys())\n",
        "    self.train = train\n",
        "    # self.indeces = indeces\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  #def getLabel(self, anchor_img):\n",
        "    \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    anchor_path = self.images[index]\n",
        "    anchor_label = self.labeledImages[anchor_path]\n",
        "    anchor_img = PIL.Image.open(anchor_path)\n",
        "\n",
        "    if self.train:\n",
        "      # get positive image path\n",
        "      positive_list = [item for item in self.images if self.labeledImages[item] == anchor_label and item != anchor_path]\n",
        "      positive_path = random.choice(positive_list)\n",
        "\n",
        "      # get negative image path\n",
        "      negative_list = [item for item in self.labeledImages.keys() if item not in positive_list]\n",
        "      negative_path = random.choice(negative_list)\n",
        "\n",
        "      # get images from paths\n",
        "      positive_img = PIL.Image.open(positive_path)\n",
        "      negative_img = PIL.Image.open(negative_path)\n",
        "\n",
        "    # transform images\n",
        "      if self.transform:\n",
        "        anchor_img = self.transform(anchor_img)\n",
        "        positive_img = self.transform(positive_img)\n",
        "        negative_img = self.transform(negative_img)\n",
        "\n",
        "      return anchor_img, positive_img, negative_img, anchor_label\n",
        "\n",
        "    else:\n",
        "      if self.transform:\n",
        "        anchor_img = self.transform(anchor_img)\n",
        "      return anchor_img, anchor_label"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRDiltBqf_Zj"
      },
      "source": [
        "lfw_dataset = LFWDataset(labeledImages)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpDxICRVGLeQ",
        "outputId": "65a2dc0f-e95f-4f45-c943-4a2bf541cc1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compute dataset sizes\n",
        "num_data = len(lfw_dataset)\n",
        "print(f\"Num. samples: {num_data}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num. samples: 9164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5J_rCfiGUQ4"
      },
      "source": [
        "# List of indexes on the dataset\n",
        "list_idx = list(range(num_data))\n",
        "random.shuffle(list_idx)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hKL7O-wwwbK"
      },
      "source": [
        "#test fraction\n",
        "test_frac = 0.1\n",
        "# Compute number of samples\n",
        "num_test = int(num_data*test_frac)\n",
        "num_data = num_data - num_test\n",
        "# Split set\n",
        "test_idx = list_idx[num_data:]\n",
        "list_idx = list_idx[:num_data]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiWIKg3MGf1u"
      },
      "source": [
        "# Split \n",
        "test_dataset = Subset(lfw_dataset, test_idx)\n",
        "dataset = Subset(lfw_dataset, list_idx)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHOgvL_uGJvZ"
      },
      "source": [
        "#validation fraction\n",
        "val_frac = 0.1\n",
        "# Compute number of samples\n",
        "num_val = int(num_data*val_frac)\n",
        "num_data = num_data - num_val\n",
        "# Split set\n",
        "val_idx = list_idx[num_data:]\n",
        "list_idx = list_idx[:num_data]"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZuNH2ycGrhS"
      },
      "source": [
        "# Split train_dataset into training and validation\n",
        "val_dataset = Subset(lfw_dataset, val_idx)\n",
        "train_dataset = Subset(lfw_dataset, list_idx)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Sg_0SpRcUay"
      },
      "source": [
        "train_dataset.dataset.transform = train_transform\n",
        "test_dataset.dataset.transform = test_transform\n",
        "val_dataset.dataset.transform = test_transform\n",
        "test_dataset.dataset.train = False\n",
        "val_dataset.dataset.train = False"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX1XMKn-HaUH",
        "outputId": "e7e7de44-e556-4a7f-84e9-09087ee40753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "num_train = len(train_dataset)\n",
        "num_test = len(test_dataset)\n",
        "num_val = len(val_dataset)\n",
        "\n",
        "print(f\"Num. training samples: {num_train}\")\n",
        "print(f\"Num. test samples: {num_test}\")\n",
        "print(f\"Num. val samples: {num_val}\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num. training samples: 7424\n",
            "Num. test samples: 916\n",
            "Num. val samples: 824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpX2l2-cGugC"
      },
      "source": [
        "# Define loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=4, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=64, num_workers=4, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=64, num_workers=4, shuffle=False)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y02ZlSDbG6ur"
      },
      "source": [
        "# Define dictionary of loaders\n",
        "loaders = {\"train\": train_loader,\n",
        "           \"val\": val_loader,\n",
        "           \"test\": test_loader}"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5az1HtKhHAXJ"
      },
      "source": [
        "### Dynamic Triplet Loss implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_u9gNzvG8bu"
      },
      "source": [
        "class TripletLoss(nn.Module):\n",
        "  def __init__(self, margin = 0.2):\n",
        "    super(TripletLoss, self).__init__()\n",
        "    self.margin = margin\n",
        "\n",
        "  def pairwise_dist(x, y):\n",
        "    dist = torch.sqrt((x - y).pow(2).sum(1))\n",
        "    return dist\n",
        "\n",
        "  def forward(self, anchor, positive, negative):\n",
        "    margin = self.margin\n",
        "    pos_dist = pairwise_dist(anchor, positive)\n",
        "    print(pos_dist, pos_dist.shape)\n",
        "    neg_dist = pairwise_dist(anchor, negative)\n",
        "    print(neg_dist)\n",
        "    loss = pos_dist - neg_dist + margin\n",
        "    loss = torch.clamp(loss, min = 0.0).sum\n",
        "    return loss"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5O85S5HHQ0q"
      },
      "source": [
        "### CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rStJHLI-jPmk",
        "outputId": "e4242bb0-a50f-4c65-f29a-0361ae9e7bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(set(labeledImages.values()))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1680"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4pIACoM8nGj"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 150, kernel_size=5, padding=0, stride=1),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(150),\n",
        "            # nn.Dropout(0.3),\n",
        "\n",
        "            nn.Conv2d(150, 256, kernel_size=5, padding=0, stride=1),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            # nn.Dropout(0.3)\n",
        "          \n",
        "            nn.Conv2d(256, 256, kernel_size=5, padding=0, stride=1),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            # nn.Dropout(0.3)\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=5, padding=0, stride=1),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            # nn.Dropout(0.3)\n",
        "        )\n",
        "  \n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(8192, 4096),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(4096, 1680)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3KzLadIApu8"
      },
      "source": [
        "### Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-LIlB5-37Jd"
      },
      "source": [
        "def train(epochs, dev, lr = 0.001):\n",
        "  try:\n",
        "    model = CNN()\n",
        "    model.to(dev)\n",
        "    print(model)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
        "    # history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "      # sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "      for split in [\"train\", \"val\", \"test\"]:\n",
        "        for step,(anchor_img, positive_img, negative_img, anchor_label) in enumerate(loaders[split]):\n",
        "\n",
        "          # Move to CUDA\n",
        "          anchor_img = anchor_img.to(device)\n",
        "          positive_img = positive_img.to(device)\n",
        "          negative_img = negative_img.to(device)\n",
        "\n",
        "          # Reset gradients\n",
        "          optimizer.zero_grad()\n",
        "          # Compute output\n",
        "          anchor_out = model(anchor_img)\n",
        "          positive_out = model(positive_img)\n",
        "          negative_out = model(negative_img)\n",
        "\n",
        "          loss = TripletLoss(anchor_out, positive_out, negative_out)\n",
        "\n",
        "          # Update loss\n",
        "          sum_loss[split] += l\n",
        "\n",
        "          # Check parameter update\n",
        "          if split == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "      # Compute epoch loss\n",
        "      epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
        "      # Update history\n",
        "      for split in [\"train\", \"val\", \"test\"]:\n",
        "        history_loss[split].append(epoch_loss[split])\n",
        "      \n",
        "      # Print info\n",
        "      print(f\"Epoch {epoch+1}:\",\n",
        "            f\"TrL={epoch_loss['train']:.4f},\"\n",
        "            f\"VL={epoch_loss['val']:.4f},\",\n",
        "            f\"TeL={epoch_loss['test']:.4f},\"\n",
        "            ) \n",
        "  except KeyboardInterrupt:\n",
        "    print(\"Interrupted\")\n",
        "  finally:\n",
        "    # Plot loss\n",
        "    plt.title(\"Loss\")\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "      plt.plot(history_loss[split], label=split)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVA1Be_TBx5u",
        "outputId": "c3decca2-25be-4ade-ab1d-c1d7e67b5122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "train(100, dev, 0.001)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (conv_layers): Sequential(\n",
            "    (0): Conv2d(3, 150, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): PReLU(num_parameters=1)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): Conv2d(150, 256, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (5): PReLU(num_parameters=1)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (9): PReLU(num_parameters=1)\n",
            "    (10): MaxPool2d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (13): PReLU(num_parameters=1)\n",
            "    (14): MaxPool2d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (fc_layers): Sequential(\n",
            "    (0): Linear(in_features=8192, out_features=4096, bias=True)\n",
            "    (1): PReLU(num_parameters=1)\n",
            "    (2): Linear(in_features=4096, out_features=1680, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-93ec95699b1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-71-cfa0b35c6dd1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, dev, lr)\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0;31m# sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0;31m# Move to CUDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
          ]
        }
      ]
    }
  ]
}